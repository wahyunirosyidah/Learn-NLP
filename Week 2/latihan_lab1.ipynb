{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data set\n",
    "#imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "#load data setnya yang sudah didownload manual\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True, data_dir=\"./data/\", download=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/plain_text/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
      "    classification containing substantially more data than previous benchmark\n",
      "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
      "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Plain text\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_dir='data\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=129.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#tampilkan informasi tentang dataset\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, 'test': <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, 'unsupervised': <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}\n"
     ]
    }
   ],
   "source": [
    "print(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#with_info=True : menerima informasi tambahan dari dataset\n",
    "#as_supervised=True : konversi data ke bentuk yang lebih sederhana (input, label) agar mudah digunakan dalam supervised learning\n",
    "#download=False berarti kita tidak perlu mengunduh ulang dataset ini karena sudah tersedia secara lokal.\n",
    "\n",
    "#melihat satu contoh ulasan\n",
    "single_example=list(imdb['train'].take(1))[0]\n",
    "\n",
    "#take(1) : mengambil 1 contoh \n",
    "# list()[0] : konversi jadi list dan ambil item pertama\n",
    "\n",
    "#tampilkan hasil\n",
    "print(single_example[0]) #kalau atasnya 0 berarti ini juga 0\n",
    "\n",
    "# [1] = positive review\n",
    "# [0] = negative review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pisah(split) menjadi data train dan data test\n",
    "#masing-masing ada 25000 data\n",
    "\n",
    "# Memisahkan review dan label untuk dataset train dan test\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "#ambil review  dari data train tanpa melihat label\n",
    "#map() : membaca setiap (review, label) dari data\n",
    "train_reviews = train_data.map(lambda review, label:review)\n",
    "train_labels = train_data.map(lambda review, label:label)\n",
    "\n",
    "test_reviews = test_data.map(lambda review, label:review)\n",
    "test_labels = test_data.map(lambda review, label:label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 120\n",
    "EMBEDDING_DIM = 16\n",
    "PADDING_TYPE = 'pre'\n",
    "TRUNC_TYPE = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buat layer\n",
    "vectorize_layer=tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "\n",
    "vectorize_layer.adapt(train_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fungsi padding : memastikan setiap urutan teks memiliki panjang yang sama\n",
    "def padding_func(sequences):\n",
    "    #batch:sekelompok data yang diproses bersamaan\n",
    "    \n",
    "    #membuat batch  \n",
    "    #sequences.cardinality(): menghitung berapa banyak item yang ada dalam dataset sequences\n",
    "    sequences=sequences.ragged_batch(batch_size=sequences.cardinality())\n",
    "    \n",
    "    #mengambil satu batch (berisi banyak elemen data) dari data?\n",
    "    sequences = sequences.get_single_element()\n",
    "    \n",
    "    padded_sequences=tf.keras.utils.pad_sequences(sequences.numpy(), #ubah bentuk data agar bisa diolah\n",
    "                                                  maxlen=120, #panjang maksimal urutan 120\n",
    "                                                  truncating='post', #memotong urutan dari belakang jika terlalu panjang\n",
    "                                                  padding='pre') #menambahkan padding di depan jika terlalu pendek\n",
    "    \n",
    "    padded_sequences=tf.data.Dataset.from_tensor_slices(padded_sequences) \n",
    "    #tadi kan data diubah jadi numpy, sekarang diubah jadi tensorflow lagi\n",
    "    \n",
    "    # Padding dilakukan setelah sequence dibentuk oleh TextVectorization\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setelah ubah kalimat string menjadi integer, apply method padding untuk sama ratakan urutan\n",
    "\n",
    "# Menerapkan vectorization dan padding\n",
    "train_sequences = train_reviews.map(lambda text:vectorize_layer(text)).apply(padding_func)\n",
    "test_sequences = test_reviews.map(lambda text:vectorize_layer(text)).apply(padding_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   0    0    0    0   11   14   34  412  384   18   90   28    1    8\n",
      "   33 1322 3560   42  487    1  191   24   85  152   19   11  217  316\n",
      "   28   65  240  214    8  489   54   65   85  112   96   22 5596   11\n",
      "   93  642  743   11   18    7   34  394 9522  170 2464  408    2   88\n",
      " 1216  137   66  144   51    2    1 7558   66  245   65 2870   16    1\n",
      " 2860    1    1 1426 5050    3   40    1 1579   17 3560   14  158   19\n",
      "    4 1216  891 8040    8    4   18   12   14 4059    5   99  146 1241\n",
      "   10  237  704   12   48   24   93   39   11 7339  152   39 1322    1\n",
      "   50  398   10   96 1155  851  141    9], shape=(120,), dtype=int32)\n",
      "\n",
      "tf.Tensor(\n",
      "[   0    0    0    0    0    0    0    0   10   26   75  617    6  776\n",
      " 2355  299   95   19   11    7  604  662    6    4 2129    5  180  571\n",
      "   63 1403  107 2410    3 3905   21    2    1    3  252   41 4781    4\n",
      "  169  186   21   11 4259   10 1507 2355   80    2   20   14 1973    2\n",
      "  114  943   14 1740 1300  594    3  356  180  446    6  596   19   17\n",
      "   57 1775    5   49   14 4002   98   42  134   10  934   10  194   26\n",
      " 1026  171    5    2   20   19   10  284    2 2065    5    9    3  279\n",
      "   41  446    6  596    5   30  200    1  201   99  146 4525   16  229\n",
      "  329   10  175  368   11   20   31   32], shape=(120,), dtype=int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View 2 training sequences and its labels\n",
    "for example in train_sequences.take(2):\n",
    "  print(example)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menggabungkan train_sequences, train_labels menjadi (input, label) dalam satu dataset\n",
    "#seperti membuat 2 kolom\n",
    "train_dataset_vectorized=tf.data.Dataset.zip(train_sequences, train_labels)\n",
    "\n",
    "test_dataset_vectorized=tf.data.Dataset.zip(test_sequences, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(120,), dtype=int32, numpy=\n",
      "array([   0,    0,    0,    0,   11,   14,   34,  412,  384,   18,   90,\n",
      "         28,    1,    8,   33, 1322, 3560,   42,  487,    1,  191,   24,\n",
      "         85,  152,   19,   11,  217,  316,   28,   65,  240,  214,    8,\n",
      "        489,   54,   65,   85,  112,   96,   22, 5596,   11,   93,  642,\n",
      "        743,   11,   18,    7,   34,  394, 9522,  170, 2464,  408,    2,\n",
      "         88, 1216,  137,   66,  144,   51,    2,    1, 7558,   66,  245,\n",
      "         65, 2870,   16,    1, 2860,    1,    1, 1426, 5050,    3,   40,\n",
      "          1, 1579,   17, 3560,   14,  158,   19,    4, 1216,  891, 8040,\n",
      "          8,    4,   18,   12,   14, 4059,    5,   99,  146, 1241,   10,\n",
      "        237,  704,   12,   48,   24,   93,   39,   11, 7339,  152,   39,\n",
      "       1322,    1,   50,  398,   10,   96, 1155,  851,  141,    9])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "\n",
      "(<tf.Tensor: shape=(120,), dtype=int32, numpy=\n",
      "array([   0,    0,    0,    0,    0,    0,    0,    0,   10,   26,   75,\n",
      "        617,    6,  776, 2355,  299,   95,   19,   11,    7,  604,  662,\n",
      "          6,    4, 2129,    5,  180,  571,   63, 1403,  107, 2410,    3,\n",
      "       3905,   21,    2,    1,    3,  252,   41, 4781,    4,  169,  186,\n",
      "         21,   11, 4259,   10, 1507, 2355,   80,    2,   20,   14, 1973,\n",
      "          2,  114,  943,   14, 1740, 1300,  594,    3,  356,  180,  446,\n",
      "          6,  596,   19,   17,   57, 1775,    5,   49,   14, 4002,   98,\n",
      "         42,  134,   10,  934,   10,  194,   26, 1026,  171,    5,    2,\n",
      "         20,   19,   10,  284,    2, 2065,    5,    9,    3,  279,   41,\n",
      "        446,    6,  596,    5,   30,  200,    1,  201,   99,  146, 4525,\n",
      "         16,  229,  329,   10,  175,  368,   11,   20,   31,   32])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View 2 training sequences and its labels\n",
    "for example in train_dataset_vectorized.take(2):\n",
    "  print(example)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perbedaan Utama\n",
    "- Bagian Pertama melakukan preprocessing untuk mengubah teks mentah menjadi angka dan menambahkan padding agar panjangnya sama.\n",
    "- Bagian Kedua menggabungkan data yang sudah diproses ini (train_sequences) dengan labelnya (train_labels) dalam satu dataset.\n",
    "  \n",
    "#### Kesimpulan\n",
    "Bagian Pertama adalah langkah preprocessing, sedangkan Bagian Kedua adalah langkah untuk menyusun dataset akhir ((input, label)) yang siap dilatih oleh model. Jika preprocessing tidak dilakukan terlebih dahulu, model tidak akan bisa membaca input teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyiapkan dataset untuk training\n",
    "SHUFFLE_BUFFER_SIZE=1000\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE=32\n",
    "\n",
    "#Optimizer dataset untuk ditraining\n",
    "train_dataset_final=(train_dataset_vectorized\n",
    "                     .cache()\n",
    "                     .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "                     .prefetch(PREFETCH_BUFFER_SIZE)\n",
    "                     .batch(BATCH_SIZE))\n",
    "\n",
    "test_dataset_final=(test_dataset_vectorized\n",
    "                     .cache()\n",
    "                     .prefetch(PREFETCH_BUFFER_SIZE)\n",
    "                     .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(120,)),\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "    \n",
    "    #jadikan array 1D\n",
    "    #tf.keras.layers.Flatten() ,\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    \n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatten akan menghasilkan shape yang besar\n",
    "- untuk alternatif, bisa menggunakan \n",
    "  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "  \n",
    "  menghasilkan array 1D dengan shape lebih kecil yang lebih simple dan lebih cepat diproses.\n",
    "\n",
    "Walaupun Flatten lebih lambat, tapi flatten lebih akurat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │       \u001b[38;5;34m160,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m102\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m7\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,109</span> (625.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m160,109\u001b[0m (625.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,109</span> (625.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m160,109\u001b[0m (625.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6633 - loss: 0.6390 - val_accuracy: 0.4987 - val_loss: 0.9539\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8442 - loss: 0.3735 - val_accuracy: 0.5022 - val_loss: 1.2554\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8760 - loss: 0.2981 - val_accuracy: 0.5003 - val_loss: 1.4329\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8946 - loss: 0.2579 - val_accuracy: 0.4985 - val_loss: 1.5577\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9111 - loss: 0.2291 - val_accuracy: 0.4980 - val_loss: 1.6641\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9206 - loss: 0.2088 - val_accuracy: 0.4988 - val_loss: 1.7778\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9285 - loss: 0.1916 - val_accuracy: 0.4975 - val_loss: 1.8786\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9356 - loss: 0.1795 - val_accuracy: 0.4999 - val_loss: 1.9963\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9419 - loss: 0.1658 - val_accuracy: 0.5008 - val_loss: 2.1104\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9480 - loss: 0.1511 - val_accuracy: 0.5003 - val_loss: 2.2093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f7d201e690>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "model.fit(train_dataset_final,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=test_dataset_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menggunakan Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(120,)),\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "    \n",
    "    #jadikan array 1D\n",
    "    tf.keras.layers.Flatten() ,\n",
    "    #tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    \n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6326 - loss: 0.6093 - val_accuracy: 0.4990 - val_loss: 1.2132\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8789 - loss: 0.2925 - val_accuracy: 0.4971 - val_loss: 1.6111\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9544 - loss: 0.1376 - val_accuracy: 0.4988 - val_loss: 2.0734\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0371 - val_accuracy: 0.4960 - val_loss: 2.5432\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0108 - val_accuracy: 0.4974 - val_loss: 2.9335\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0039 - val_accuracy: 0.4964 - val_loss: 3.1566\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.4980 - val_loss: 3.5009\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.4596e-04 - val_accuracy: 0.4975 - val_loss: 3.7447\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.0501e-04 - val_accuracy: 0.4976 - val_loss: 3.9534\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.8385e-04 - val_accuracy: 0.4977 - val_loss: 4.1556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f7d38f6690>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "EPOCHS=10\n",
    "model2.fit(train_dataset_final,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=test_dataset_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisasi Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lapisan embedding berfungsi untuk mengonversi kata-kata atau token yang diinput menjadi representasi vektor berukuran tetap (dimensi embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "#mengakses lapisan pertama (layers[0]) dari model, sebagai lapisan embedding?\n",
    "\n",
    "# Mengambil bobot dari lapisan embedding\n",
    "embedding_layer=model.layers[0]\n",
    "\n",
    "#lapisan [0] : lapisan embedding\n",
    "#lapisan [1] : lapisan dense\n",
    "#lapisan [2] : lapisan output\n",
    "\n",
    "#mengembil bobot dari lapisan embedding\n",
    "embedding_weights =  embedding_layer.get_weights()[0]\n",
    "\n",
    "print(embedding_weights.shape) #shape: (covab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contoh\n",
    "\n",
    "Jika vocab_size = 5000 dan embedding_dim = 64, maka ukuran dari embedding_weights akan menjadi (5000, 64), yang berarti terdapat 5000 kata dalam kosakata, masing-masing direpresentasikan oleh vektor embedding berdimensi 64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Membuka file dan menyimpan data\n",
    "\n",
    "#vecs.tsv: simpan vector embedding dari setiap kata\n",
    "#bukan dan write(bisa menulis di file)\n",
    "out_v=io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "#meta.tsv: menyimpan kata-kata atau \"metadata\"-nya.\n",
    "out_m=io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "#mengambil semua kata di vocabulary dari vectorize_layer\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "\n",
    "#Menulis Kata dan Vektor Embedding ke File\n",
    "\n",
    "#iterasi kata pertama sampai terakhir di vocabulary\n",
    "#langsung 1 karena 0 itu padding\n",
    "for word_num in range(1, len(vocabulary)):\n",
    "   \n",
    "    word_name = vocabulary[word_num] #Mengambil Nama Kata\n",
    "    word_embedding = embedding_weights[word_num] #Mengambil Vektor Embedding Kata\n",
    "    \n",
    "    #Menulis ke File\n",
    "    out_m.write(word_name + '\\n') \n",
    "    out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mulai dari 1 agar tidak menuliskan \"kata kosong\" yang biasa ada di posisi 0 untuk padding.\n",
    "- word_name berisi kata yang akan ditulis ke file meta.tsv\n",
    "- word_embedding adalah array yang menyimpan representasi vektor dari word_name.\n",
    "- Menulis word_name ke file meta.tsv di setiap baris, agar file ini berisi daftar kata.\n",
    "- Menulis vektor word_embedding ke vecs.tsv. Setiap nilai dalam array word_embedding dipisahkan dengan \\t (tab) agar vektor ini berbentuk tab-delimited pada setiap barisnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogi\n",
    "Anggap saja vocabulary ini seperti kamus di mana setiap kata punya definisi. Di sini, meta.tsv adalah daftar kata yang berfungsi seperti daftar isi buku, dan vecs.tsv adalah bagian yang memuat penjelasan atau deskripsi rinci (vektor embedding) dari setiap kata.\n",
    "\n",
    "Contohnya, jika vocabulary berisi [\"\", \"good\", \"bad\", \"excellent\", \"terrible\"], maka:\n",
    "\n",
    "- meta.tsv akan berisi:\n",
    "  - good\n",
    "  - bad\n",
    "  - excellent\n",
    "  - terrible\n",
    "  \n",
    "- vecs.tsv mungkin akan berisi sesuatu seperti:\n",
    "  \n",
    "  0.1    -0.4   0.6    ...   (embedding untuk \"good\")\n",
    "  \n",
    "  -0.3   0.8   -0.2    ...   (embedding untuk \"bad\")\n",
    " \n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setiap kata memang memiliki satu token (atau indeks unik) dalam model, tetapi embedding adalah representasi numerik dari kata yang menangkap maknanya dalam bentuk vektor. Jadi, meskipun satu kata hanya memiliki satu token, embedding adalah array (vektor) dari beberapa nilai, yang semuanya bersama-sama menggambarkan fitur atau arti kata tersebut di dalam ruang multidimensi.\n",
    "\n",
    "#### Mengapa Setiap Kata Memiliki Banyak Nilai Embedding?\n",
    "\n",
    "Embedding dalam NLP bertujuan untuk menangkap makna atau konteks kata dalam ruang vektor yang kaya informasi. Setiap nilai dalam vektor embedding ini berkontribusi untuk merepresentasikan karakteristik tertentu dari kata tersebut. Misalnya, jika embedding berukuran 16 (Jumlah dimensinya), maka setiap kata akan memiliki vektor berisi 16 angka. Angka-angka ini tidak mewakili kata dalam bentuk satu nilai saja, tetapi sebagai kumpulan fitur-fitur (misalnya: asosiasi positif/negatif, kategori umum, atau hubungan dengan kata lain).\n",
    "\n",
    "#### Cara Embedding Bekerja\n",
    "Misalnya, jika kita punya kata “good,” dan embedding berukuran 16, model akan memberikan representasi yang tampak seperti ini:\n",
    "\n",
    "[0.1, -0.2, 0.8, ..., -0.4]\n",
    "\n",
    "Setiap angka dalam vektor ini mewakili aspek tertentu dari kata “good.” Vektor ini akan berbeda untuk kata lain, misalnya “bad,” karena model membedakan makna masing-masing kata berdasarkan pola hubungan mereka di dalam teks yang dilatihkan.\n",
    "\n",
    "#### Analogi\n",
    "Pikirkan embedding seperti menggambarkan karakter seseorang dalam berbagai dimensi (misalnya, kecerdasan, kebaikan, humor, kepercayaan, dll.). Seseorang mungkin memiliki nilai berbeda di masing-masing dimensi tersebut, dan ketika kita melihat keseluruhan vektor (semua dimensi), kita mendapatkan gambaran lengkap tentang karakter orang tersebut. Begitu juga, embedding sebuah kata memetakan kata dalam berbagai dimensi yang, secara keseluruhan, memberikan “makna” kata tersebut dalam konteks model.\n",
    "\n",
    "Jadi, embedding bukan sekadar satu angka, tapi satu \"potret\" vektor multidimensi yang menggambarkan arti sebuah kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
