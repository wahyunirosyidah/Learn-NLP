{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data set\n",
    "#imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "#load data setnya yang sudah didownload manual\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True, data_dir=\"./data/\", download=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/plain_text/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
      "    classification containing substantially more data than previous benchmark\n",
      "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
      "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Plain text\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_dir='data\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=129.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#tampilkan informasi tentang dataset\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, 'test': <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, 'unsupervised': <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}\n"
     ]
    }
   ],
   "source": [
    "print(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#with_info=True : menerima informasi tambahan dari dataset\n",
    "#as_supervised=True : konversi data ke bentuk yang lebih sederhana (input, label) agar mudah digunakan dalam supervised learning\n",
    "#download=False berarti kita tidak perlu mengunduh ulang dataset ini karena sudah tersedia secara lokal.\n",
    "\n",
    "#melihat satu contoh ulasan\n",
    "single_example=list(imdb['train'].take(1))[0]\n",
    "\n",
    "#take(1) : mengambil 1 contoh \n",
    "# list()[0] : konversi jadi list dan ambil item pertama\n",
    "\n",
    "#tampilkan hasil\n",
    "print(single_example[0]) #kalau atasnya 0 berarti ini juga 0\n",
    "\n",
    "# [1] = positive review\n",
    "# [0] = negative review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pisah(split) menjadi data train dan data test\n",
    "#masing-masing ada 25000 data\n",
    "\n",
    "# Memisahkan review dan label untuk dataset train dan test\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "#ambil review  dari data train tanpa melihat label\n",
    "#map() : membaca setiap (review, label) dari data\n",
    "train_reviews = train_data.map(lambda review, label:review)\n",
    "train_labels = train_data.map(lambda review, label:label)\n",
    "\n",
    "test_reviews = test_data.map(lambda review, label:review)\n",
    "test_labels = test_data.map(lambda review, label:label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buat layer\n",
    "vectorize_layer=tf.keras.layers.TextVectorization(max_tokens=10000)\n",
    "\n",
    "vectorize_layer.adapt(train_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fungsi padding : memastikan setiap urutan teks memiliki panjang yang sama\n",
    "def padding_func(sequences):\n",
    "    #batch:sekelompok data yang diproses bersamaan\n",
    "    \n",
    "    #membuat batch  \n",
    "    #sequences.cardinality(): menghitung berapa banyak item yang ada dalam dataset sequences\n",
    "    sequences=sequences.ragged_batch(batch_size=sequences.cardinality())\n",
    "    \n",
    "    #mengambil satu batch (berisi banyak elemen data) dari data?\n",
    "    sequences = sequences.get_single_element()\n",
    "    \n",
    "    padded_sequences=tf.keras.utils.pad_sequences(sequences.numpy(), #ubah bentuk data agar bisa diolah\n",
    "                                                  maxlen=120, #panjang maksimal urutan 120\n",
    "                                                  truncating='post', #memotong urutan dari belakang jika terlalu panjang\n",
    "                                                  padding='pre') #menambahkan padding di depan jika terlalu pendek\n",
    "    \n",
    "    padded_sequences=tf.data.Dataset.from_tensor_slices(padded_sequences) \n",
    "    #tadi kan data diubah jadi numpy, sekarang diubah jadi tensorflow lagi\n",
    "    \n",
    "    # Padding dilakukan setelah sequence dibentuk oleh TextVectorization\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setelah ubah kalimat string menjadi integer, apply method padding untuk sama ratakan urutan\n",
    "\n",
    "# Menerapkan vectorization dan padding\n",
    "train_sequences = train_reviews.map(lambda text:vectorize_layer(text)).apply(padding_func)\n",
    "test_sequences = test_reviews.map(lambda text:vectorize_layer(text)).apply(padding_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menggabungkan train_sequences, train_labels menjadi (input, label) dalam satu dataset\n",
    "#seperti membuat 2 kolom\n",
    "train_dataset_vectorized=tf.data.Dataset.zip(train_sequences, train_labels)\n",
    "\n",
    "test_dataset_vectorized=tf.data.Dataset.zip(test_sequences, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyiapkan dataset untuk training\n",
    "SHUFFLE_BUFFER_SIZE=1000\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE=32\n",
    "\n",
    "train_dataset_final=(train_dataset_vectorized\n",
    "                     .cache()\n",
    "                     .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "                     .prefetch(PREFETCH_BUFFER_SIZE)\n",
    "                     .batch(BATCH_SIZE))\n",
    "\n",
    "test_dataset_final=(test_dataset_vectorized\n",
    "                     .cache()\n",
    "                     .prefetch(PREFETCH_BUFFER_SIZE)\n",
    "                     .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 120\n",
    "EMBEDDING_DIM = 16\n",
    "PADDING_TYPE = 'pre'\n",
    "TRUNC_TYPE = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(120,)),\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "    \n",
    "    #jadikan array 1D\n",
    "    #tf.keras.layers.Flatten() ,\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    \n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatten akan menghasilkan shape yang besar\n",
    "- untuk alternatif, bisa menggunakan \n",
    "  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "  \n",
    "  menghasilkan array 1D dengan shape lebih kecil yang lebih simple dan lebih cepat diproses.\n",
    "\n",
    "Walaupun Flatten lebih lambat, tapi flatten lebih akurat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │       \u001b[38;5;34m160,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m102\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m7\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,109</span> (625.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m160,109\u001b[0m (625.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,109</span> (625.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m160,109\u001b[0m (625.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6593 - loss: 0.6382 - val_accuracy: 0.4980 - val_loss: 0.9651\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8449 - loss: 0.3716 - val_accuracy: 0.4993 - val_loss: 1.2588\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8781 - loss: 0.2961 - val_accuracy: 0.4996 - val_loss: 1.4152\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8981 - loss: 0.2559 - val_accuracy: 0.4980 - val_loss: 1.5612\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9108 - loss: 0.2301 - val_accuracy: 0.4978 - val_loss: 1.6770\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9213 - loss: 0.2096 - val_accuracy: 0.4981 - val_loss: 1.7802\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9297 - loss: 0.1906 - val_accuracy: 0.4987 - val_loss: 1.8907\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1779 - val_accuracy: 0.4992 - val_loss: 2.0146\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9425 - loss: 0.1642 - val_accuracy: 0.4990 - val_loss: 2.1370\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9462 - loss: 0.1546 - val_accuracy: 0.4984 - val_loss: 2.2537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x274381e4680>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "model.fit(train_dataset_final,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=test_dataset_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menggunakan Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(120,)),\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "    \n",
    "    #jadikan array 1D\n",
    "    tf.keras.layers.Flatten() ,\n",
    "    #tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    \n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6096 - loss: 0.6219 - val_accuracy: 0.4959 - val_loss: 1.2499\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8725 - loss: 0.3028 - val_accuracy: 0.4978 - val_loss: 1.5472\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9426 - loss: 0.1630 - val_accuracy: 0.4994 - val_loss: 1.9402\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9896 - loss: 0.0534 - val_accuracy: 0.4987 - val_loss: 2.3640\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0234 - val_accuracy: 0.4984 - val_loss: 2.6876\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0110 - val_accuracy: 0.4981 - val_loss: 2.9397\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9991 - loss: 0.0062 - val_accuracy: 0.4986 - val_loss: 3.2662\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0023 - val_accuracy: 0.4984 - val_loss: 3.5647\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.3660e-04 - val_accuracy: 0.4980 - val_loss: 3.8309\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.4772e-04 - val_accuracy: 0.4981 - val_loss: 4.0362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27437becdd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "EPOCHS=10\n",
    "model2.fit(train_dataset_final,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=test_dataset_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lapisan embedding berfungsi untuk mengonversi kata-kata atau token yang diinput menjadi representasi vektor berukuran tetap (dimensi embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "#mengakses lapisan pertama (layers[0]) dari model, sebagai lapisan embedding?\n",
    "\n",
    "# Mengambil bobot dari lapisan embedding\n",
    "embedding_layer=model.layers[0]\n",
    "\n",
    "#lapisan [0] : lapisan embedding\n",
    "#lapisan [1] : lapisan dense\n",
    "#lapisan [2] : lapisan output\n",
    "\n",
    "#mengembil bobot dari lapisan embedding\n",
    "embedding_weights =  embedding_layer.get_weights()[0]\n",
    "\n",
    "print(embedding_weights.shape) #shape: (covab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contoh\n",
    "\n",
    "Jika vocab_size = 5000 dan embedding_dim = 64, maka ukuran dari embedding_weights akan menjadi (5000, 64), yang berarti terdapat 5000 kata dalam kosakata, masing-masing direpresentasikan oleh vektor embedding berdimensi 64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Membuka file dan menyimpan data\n",
    "\n",
    "#vecs.tsv: simpan vector embedding dari setiap kata\n",
    "#bukan dan write(bisa menulis di file)\n",
    "out_v=io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "#meta.tsv: menyimpan kata-kata atau \"metadata\"-nya.\n",
    "out_m=io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "#mengambil semua kata di vocabulary dari vectorize_layer\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "\n",
    "#Menulis Kata dan Vektor Embedding ke File\n",
    "\n",
    "#iterasi kata pertama sampai terakhir di vocabulary\n",
    "for word_num in range(1, len(vocabulary)):\n",
    "   \n",
    "    word_name = vocabulary[word_num] #Mengambil Nama Kata\n",
    "    word_embedding = embedding_weights[word_num] #Mengambil Vektor Embedding Kata\n",
    "    \n",
    "    #Menulis ke File\n",
    "    out_m.write(word_name + '\\n') \n",
    "    out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mulai dari 1 agar tidak menuliskan \"kata kosong\" yang biasa ada di posisi 0 untuk padding.\n",
    "- word_name berisi kata yang akan ditulis ke file meta.tsv\n",
    "- word_embedding adalah array yang menyimpan representasi vektor dari word_name.\n",
    "- Menulis word_name ke file meta.tsv di setiap baris, agar file ini berisi daftar kata.\n",
    "- Menulis vektor word_embedding ke vecs.tsv. Setiap nilai dalam array word_embedding dipisahkan dengan \\t (tab) agar vektor ini berbentuk tab-delimited pada setiap barisnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogi\n",
    "Anggap saja vocabulary ini seperti kamus di mana setiap kata punya definisi. Di sini, meta.tsv adalah daftar kata yang berfungsi seperti daftar isi buku, dan vecs.tsv adalah bagian yang memuat penjelasan atau deskripsi rinci (vektor embedding) dari setiap kata.\n",
    "\n",
    "Contohnya, jika vocabulary berisi [\"\", \"good\", \"bad\", \"excellent\", \"terrible\"], maka:\n",
    "\n",
    "- meta.tsv akan berisi:\n",
    "  - good\n",
    "  - bad\n",
    "  - excellent\n",
    "  - terrible\n",
    "  \n",
    "- vecs.tsv mungkin akan berisi sesuatu seperti:\n",
    "  \n",
    "  0.1    -0.4   0.6    ...   (embedding untuk \"good\")\n",
    "  \n",
    "  -0.3   0.8   -0.2    ...   (embedding untuk \"bad\")\n",
    " \n",
    "  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
