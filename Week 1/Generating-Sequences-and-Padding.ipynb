{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to convert input sentences into numeric sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korpus merupakan kumpulan teks yang tersimpan secara elektronik untuk berbagai kebutuhan penyelidikan dan penelitian, salah satunya dalam pemrosesan bahasa alami (Natural Language Processing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam konteks pemrosesan bahasa alami (Natural Language Processing), \"corpus\" merujuk pada kumpulan teks yang digunakan untuk analisis atau pelatihan model. Corpus bisa terdiri dari berbagai jenis teks, seperti artikel, buku, percakapan, atau dokumen lainnya. \n",
    "\n",
    "Berikut adalah beberapa poin penting tentang corpus:\n",
    "\n",
    "- Sumber Data: Corpus berfungsi sebagai sumber data untuk melatih model bahasa, membantu model memahami pola dan struktur bahasa.\n",
    "- Ukuran: Corpus bisa bervariasi dalam ukuran, dari beberapa kalimat hingga jutaan kata, tergantung pada tujuan analisis.\n",
    "Kualitas: Kualitas corpus sangat penting; corpus yang baik harus representatif dan relevan dengan tugas yang ingin diselesaikan.\n",
    "- Penggunaan: Dalam pelatihan model, corpus digunakan untuk mengajarkan model tentang kosakata, tata bahasa, dan konteks penggunaan kata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lab, i use the TextVectorization layer to build vocabulary from my corpus. Itu menghasilkan daftar di mana kata-kata yang lebih sering muncul memiliki indeks yang lebih rendah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 [UNK]\n",
      "2 i\n",
      "3 wahyuni\n",
      "4 wahyu\n",
      "5 supernova\n",
      "6 me\n",
      "7 love\n",
      "8 huh\n",
      "9 cantik\n",
      "10 bukan\n",
      "11 ayu\n",
      "12 am\n"
     ]
    }
   ],
   "source": [
    "#Sample inputs\n",
    "\n",
    "sentences=[\n",
    "    'Ayu cantik',\n",
    "    'Wahyuni bukan Wahyu',\n",
    "    'I love me',\n",
    "    'Am I supernova huh?'\n",
    "]\n",
    "\n",
    "#inisialisasi layer\n",
    "vectorize_layer=tf.keras.layers.TextVectorization()\n",
    "\n",
    "#Build vocabulary\n",
    "#tiap kata pada sentences disimpan dalam bentuk indeks\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "#Get the vocabulary\n",
    "vocabulary=vectorize_layer.get_vocabulary()\n",
    "\n",
    "#Print the token index\n",
    "for index, word in enumerate(vocabulary):\n",
    "    print(index, word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 1 1 0]\n",
      " [2 7 6 0]\n",
      " [2 7 1 1]], shape=(3, 4), dtype=int64)\n",
      "\n",
      " tf.Tensor([12  2  5  8], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#Memeriksa token pada suatu kalimat\n",
    "#convert sentence to integer sequences\n",
    "\n",
    "sample_sentence=[\n",
    "        'saya makan nasi',\n",
    "        'i love me',\n",
    "        'i love nasi padang'\n",
    "]\n",
    "\n",
    "sample_sentence2='am i supernova huh? '\n",
    "\n",
    "#convert\n",
    "sequences1=vectorize_layer(sample_sentence)\n",
    "sequences2=vectorize_layer(sample_sentence2)\n",
    "\n",
    "# for index, word in enumerate(sample_sentence):\n",
    "print(sequences1)\n",
    "print(\"\\n\",sequences2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given list of string inputs (such as the 4-item `sentences` list above), you will need to apply the layer to each input. There's more than one way to do this. Let's first use the `map()` method and see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk list of string inputs tertentu (seperti daftar `sentences` berisi 4 item di atas), Anda perlu menerapkan layer tersebut ke setiap masukan. Ada lebih dari satu cara untuk melakukannya. Mari kita gunakan metode `map()` terlebih dahulu dan lihat hasilnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None)>\n"
     ]
    }
   ],
   "source": [
    "#convert the list to tf.data.Dataset\n",
    "#membuat daftar dari kalimat menjadi suatu dataset\n",
    "#setiap elemen dari dataset adalah satu kalimat dari sentences\n",
    "sentences_dataset = tf.data.Dataset.from_tensor_slices(sentences)\n",
    "\n",
    "#mapping function untuk convert each inputs to integer sequence\n",
    "#aplikasikan vectorize_layer pada setiap sentence \n",
    "sequences_convert=sentences_dataset.map(vectorize_layer)\n",
    "\n",
    "print(sequences_convert)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayu cantik ----> [11  9]\n",
      "Wahyuni bukan Wahyu ----> [ 3 10  4]\n",
      "I love me ----> [2 7 6]\n",
      "Am I supernova huh? ----> [12  2  5  8]\n"
     ]
    }
   ],
   "source": [
    "for sentencenya, sequencenya in zip(sentences, sequences_convert):\n",
    "    print(f\"{sentencenya} ----> {sequencenya}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index for padding --> [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "['Ayu cantik', 'Wahyuni bukan Wahyu', 'I love me', 'Am I supernova huh?']\n",
      "[11  9]\n",
      "[ 3 10  4]\n",
      "[2 7 6]\n",
      "[12  2  5  8]\n",
      "\n",
      "Output: \n",
      "tf.Tensor(\n",
      "[[11  9  0  0]\n",
      " [ 3 10  4  0]\n",
      " [ 2  7  6  0]\n",
      " [12  2  5  8]], shape=(4, 4), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sequences_post = vectorize_layer(sentences)\n",
    "\n",
    "print('Input: ')\n",
    "print(sentences)\n",
    "[print(sequence.numpy()) for sequence in sequences_convert]\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print('Output: ')\n",
    "print(sequences_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "[11  9]\n",
      "[ 3 10  4]\n",
      "[2 7 6]\n",
      "[12  2  5  8]\n",
      "\n",
      "Output: \n",
      "[[ 0  0 11  9]\n",
      " [ 0  3 10  4]\n",
      " [ 0  2  7  6]\n",
      " [12  2  5  8]]\n"
     ]
    }
   ],
   "source": [
    "sequences_pre=tf.keras.utils.pad_sequences(sequences_convert, padding='pre')\n",
    "\n",
    "print('Input: ')\n",
    "[print(sequence.numpy()) for sequence in sequences_convert]\n",
    "print()\n",
    "\n",
    "print('Output: ')\n",
    "print(sequences_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-padding and Limit Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "truncating=memotong data jika kepanjangan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "[11  9]\n",
      "[ 3 10  4]\n",
      "[2 7 6]\n",
      "[12  2  5  8]\n",
      "\n",
      "Output: \n",
      "[[11  9  0]\n",
      " [ 3 10  4]\n",
      " [ 2  7  6]\n",
      " [ 2  5  8]]\n"
     ]
    }
   ],
   "source": [
    "sequences_post_trunc=tf.keras.utils.pad_sequences(sequences_convert, maxlen=3, padding='post')\n",
    "\n",
    "print('Input: ')\n",
    "[print(sequence.numpy()) for sequence in sequences_convert]\n",
    "print()\n",
    "\n",
    "print('Output: ')\n",
    "print(sequences_post_trunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragged Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cara lain untuk menyiapkan urutan untuk prepadding adalah dengan menyetel TextVectorization untuk menghasilkan tensor yang tidak rata. Ini berarti output tidak akan secara otomatis diberi post-padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ragged tensor adalah jenis tensor yang memungkinkan setiap elemen memiliki panjang yang berbeda. Ini sangat berguna dalam pemrosesan teks karena kalimat memiliki jumlah kata yang berbeda-beda.\n",
    "\n",
    "Kamu tidak perlu menambah nilai kosong, dan data tetap memiliki panjang yang berbeda, tetapi TensorFlow tetap bisa memprosesnya dengan cara yang lebih efisien daripada padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[11, 9], [3, 10, 4], [2, 7, 6], [12, 2, 5, 8]]>\n"
     ]
    }
   ],
   "source": [
    "#membuat layer dengan outputnya ragged tensor\n",
    "vectorize_rlayer = tf.keras.layers.TextVectorization(ragged=True)\n",
    "\n",
    "vectorize_rlayer.adapt(sentences)\n",
    "\n",
    "ragged_sequences=vectorize_rlayer(sentences)\n",
    "\n",
    "print(ragged_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".numpy() mengonversi tensor ragged menjadi array NumPy yang lebih sederhana, sehingga dapat diproses langsung oleh pad_sequences (menambahkan urutan/padding) yang berfungsi memastikan semua sequence memiliki panjang yang sama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 11  9]\n",
      " [ 0  3 10  4]\n",
      " [ 0  2  7  6]\n",
      " [12  2  5  8]]\n"
     ]
    }
   ],
   "source": [
    "# Pre-pad the sequences in the ragged tensor\n",
    "sequencesr_pre=tf.keras.utils.pad_sequences(ragged_sequences.numpy())\n",
    "\n",
    "print(sequencesr_pre)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasilnya adalah ragged tensor yang berisi urutan angka untuk setiap kalimat, dengan panjang yang sesuai dengan jumlah kata dalam kalimat tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-vocabulary tokens (OOV)\n",
    "\n",
    "oov --> [1]\n",
    "\n",
    "Input kata yang tidak dikenali oleh vocabulary (oov) akan ditandai dengan index [1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really love you, but ----> [2 1 7 1 1]\n",
      "we are lie ----> [1 1 1 0 0]\n",
      "I love the scent ----> [2 7 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "sentences_with_oov=[\n",
    "    'i really love you, but',\n",
    "    'we are lie',\n",
    "    'I love the scent'\n",
    "]\n",
    "\n",
    "sequences_with_oov= vectorize_layer(sentences_with_oov)\n",
    "\n",
    "for sentence, sequence in zip(sentences_with_oov, sequences_with_oov):\n",
    "    print(f\"{sentence} ----> {sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
